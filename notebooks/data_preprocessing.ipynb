{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset, load_dataset, concatenate_datasets\n",
    "import torch\n",
    "\n",
    "# variables\n",
    "content_system = 'Your name is Trip Assistant bot. You are a useful assistant as an AI chatbot to complete the tasks of the airline and hotel booking system. You can search, book, cancel and update flights and hotel rooms. The assistant is helpful, resourceful, smart and very friendly. If the user wants, he can talk about anything as soon as the assistant finishes filling in the following fields from the user: username, (when booking an air ticket: place of departure, destination), (when booking a hotel: location, number of stars), travel dates, trip budget. Any additional preferences or constraints the user may have. Get user detail step by step in proper conversion.'\n",
    "model_name = 'KvrParaskevi/Llama-2-7b-Hotel-Booking-Model'\n",
    "path_hi_answers = 'additional_data/hi_answers.txt'\n",
    "path_additional_data = 'additional_data/additional_data.txt'\n",
    "path_to_save_dataset = 'hotel_fights_data'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tokenizer and datasets\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "dataset_ticket = load_dataset(\"mithlesh/Flight_Ticket_Booking_Conversion\")\n",
    "dataset_hotel = load_dataset(\"KvrParaskevi/hotel_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change data | add my instruction\n",
    "dataset_ticket = dataset_ticket.map(lambda  example: {'Instruction': content_system}, remove_columns=['__index_level_0__', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['system', 'user', 'assistant'],\n",
       "        num_rows: 2126\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change data | rename columns\n",
    "dataset_ticket = dataset_ticket.rename_column('Instruction', 'system').rename_column('Human', 'user').rename_column('Bot', 'assistant')\n",
    "dataset_ticket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening and reading the file containing responses to \"hi\" \n",
    "with open(path_hi_answers) as f:\n",
    "    hi_answers = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_cyclic_generator(input_list: list):\n",
    "    \"\"\"\n",
    "    Converts a list into a cyclic generator that infinitely yields items from the list.\n",
    "\n",
    "    Args:\n",
    "        input_list (list): The list to be converted.\n",
    "\n",
    "    Yields:\n",
    "        Any: Items from the input list, cyclically.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        for item in input_list:\n",
    "            yield item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a cyclic generator for hi answers\n",
    "hi_answers_gen = list_to_cyclic_generator(hi_answers)\n",
    "def change_answers_hi(example: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Change the assistant's response if user say 'hi'.\n",
    "    Args:\n",
    "        example (dict): A dictionary representing an example from the dataset, with 'user' and 'assistant' keys.\n",
    "    Returns:\n",
    "        dict: The modified example with the assistant's response changed.\n",
    "    \"\"\"\n",
    "    if example['user'].lower() == 'hi':\n",
    "         example[\"assistant\"] = next(hi_answers_gen)\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function change_answers_hi at 0xffff4c012af0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "Map: 100%|██████████| 2126/2126 [00:00<00:00, 25234.72 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['system', 'user', 'assistant'],\n",
       "        num_rows: 2126\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change data | hi answers, apply change_answers_hi\n",
    "dataset_ticket = dataset_ticket.map(change_answers_hi)\n",
    "dataset_ticket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_answers_thanks(example: dict) -> dict:\n",
    "     \"\"\"\n",
    "     Changes the assistant's response by removing 'Thanks for choosing Propellyr!' if specific user input conditions are met.\n",
    "     Args:\n",
    "          example (dict): A dictionary representing an example from the dataset, with 'user' and 'assistant' keys.\n",
    "     Returns:\n",
    "          dict: The modified conversation example.\n",
    "     \"\"\"\n",
    "\n",
    "    if ((example['user'].lower() == 'yes') or (example['user'].lower() == 'please finally book this ticket too with promo code'))and ('propellyr' in example['assistant'].lower()) :\n",
    "         example[\"assistant\"] =  example[\"assistant\"].replace('Thanks for choosing Propellyr!','').strip()\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2126/2126 [00:00<00:00, 29789.97 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['system', 'user', 'assistant'],\n",
       "        num_rows: 2126\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change data | remove 'Thanks for choosing Propellyr!'\n",
    "dataset_ticket = dataset_ticket.map(change_answers_thanks)\n",
    "dataset_ticket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_answers_another(example: dict)  -> dict:\n",
    "    \"\"\"\n",
    "    Changes the assistant's response based on specific user inputs.\n",
    "\n",
    "    Args:\n",
    "        example (dict): A dictionary representing an example from the dataset, with 'user' and 'assistant' keys.\n",
    "\n",
    "    Returns:\n",
    "        dict: The modified conversation example.\n",
    "    \"\"\"\n",
    "    change_dict = {\n",
    "    'mithlesh upadhyay':['mithlesh upadhyay',\"Hello Mithlesh Upadhyay! It's great to meet you. I'm Travel Booking Bot, your friendly AI assistant for flight ticket and hotel rooms booking.I'm here to help you plan the perfect trip. Are you looking to book plane tickets or a hotel room today?\"],\n",
    "    \"Who are you'\" : [\"Who are you?\", \"I'm a Travel Booking Bot designed to assist you in booking flights and plane tickets.\"],\n",
    "    \"hi, what you can do\": [\"hi, what you can do?\", \"I can assist you in searching for flights, comparing prices, and booking plane tickets for your travel needs.\"]\n",
    "    }\n",
    "    if example['user'] in change_dict:\n",
    "        example['assistant'] = change_dict[example['user']][1]\n",
    "        example['user'] = change_dict[example['user']][0]   \n",
    "    return example \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2126/2126 [00:00<00:00, 27994.16 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['system', 'user', 'assistant'],\n",
       "        num_rows: 2126\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change data | fix another  \n",
    "dataset_ticket = dataset_ticket.map(change_answers_another)\n",
    "dataset_ticket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening and reading the file with additional data\n",
    "with open(path_additional_data) as f:\n",
    "    additional_data = f.read().split('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionary to store additional data\n",
    "add_data = {'user':[], 'assistant':[], 'system':[]}\n",
    "for data in additional_data:\n",
    "    \"\"\"\n",
    "    Iterates through each data entry in additional_data.\n",
    "\n",
    "    - Splits the data into lines.\n",
    "    - Appends the system content to the 'system' list.\n",
    "    - Appends the user input (stripped of 'Q:') to the 'user' list.\n",
    "    - Appends the assistant response (stripped of 'A:') to the 'assistant' list.\n",
    "    \"\"\"\n",
    "    data = data.split('\\n')\n",
    "    add_data['system'].append(content_system)\n",
    "    add_data['user'].append(data[0].replace('Q:','').strip())\n",
    "    add_data['assistant'].append(data[1].replace('A:','').strip())\n",
    "    \n",
    "# Creating a dataset from the collected data\n",
    "add_data = Dataset.from_dict(add_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating the original ticket dataset with the additional data\n",
    "dataset_ticket = concatenate_datasets([dataset_ticket['train'], add_data])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping and transforming the hotel dataset\n",
    "dataset_hotel = dataset_hotel.map(\n",
    "    lambda example: {\n",
    "        'system': content_system,  # Assigning system content\n",
    "        'user': example['text'].split('###')[1].replace('Human:', '').strip(),  # Extracting and cleaning user input\n",
    "        'assistant': example['text'].split('###')[2].replace('Assistant:', '').strip()  # Extracting and cleaning assistant response\n",
    "    }, \n",
    "    remove_columns=['text']  # Removing the 'text' column after transformation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['system', 'user', 'assistant'],\n",
       "    num_rows: 3355\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenating the hotel and ticket datasets\n",
    "my_dataset = concatenate_datasets([dataset_hotel['train'], dataset_ticket])\n",
    "my_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_text(example: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Adds 'text' field to the example by applying chat template and tokenizing.\n",
    "\n",
    "    Args:\n",
    "        example (dict): A dictionary representing a conversation example with 'user', 'assistant', and 'system' keys.\n",
    "    Returns:\n",
    "        dict: The modified conversation example with an added 'text' field.\n",
    "    \"\"\"\n",
    "    # Constructing chat format\n",
    "\n",
    "    chat = [\n",
    "    {\n",
    "     \"role\": \"system\",\n",
    "     \"content\": example['system'],\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": example['user']},\n",
    "    {\"role\": \"assistant\", \"content\": example['assistant']}\n",
    "    ]\n",
    "    # Applying chat template and tokenizing\n",
    "    example['text'] = tokenizer.apply_chat_template( chat, tokenize=False, add_generation_prompt=True).replace('<s>','').replace('</s>','').strip()\n",
    "    return example\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3355/3355 [00:00<00:00, 13901.79 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Mapping the add_text function to the dataset to add 'text' field\n",
    "my_dataset = my_dataset.map(add_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 3355/3355 [00:00<00:00, 30312.26 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Saving the dataset\n",
    "my_dataset.save_to_disk(path_to_save_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
